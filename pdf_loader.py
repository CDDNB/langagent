import fitz  # PyMuPDF è§£æ PDF
import docx
from openai import OpenAI
import sys
import os
import json
from translate import DocumentTranslator
from mindmap import MindMapGenerator
from reading_comprehension import ReadingComprehension, word_extraction

api_key = 'sk-de619016f861434aa6a70d34eed13085'
api_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"

def extract_text_from_docx(docx_path):
    """ä» Word æ–‡ä»¶ä¸­æå–æ–‡æœ¬ï¼Œä¿ç•™æ ‡é¢˜æ ¼å¼"""
    doc = docx.Document(docx_path)
    paragraphs = []
    
    for para in doc.paragraphs:
        text = para.text.strip()
        if not text:
            continue
        paragraphs.append(text)  # æ™®é€šæ­£æ–‡æ®µè½

    return paragraphs

def extract_text_from_pdf(pdf_path):
    """ä» PDF æå–æ–‡æœ¬ï¼Œå¹¶æŒ‰æ®µè½åˆ†å‰²"""
    doc = fitz.open(pdf_path)
    paragraphs = []
    temp_paragraph = ""

    for page in doc:
        text = page.get_text("text")
        for line in text.split("\n"):
            if line.strip():
                temp_paragraph += " " + line.strip()
            else:
                if temp_paragraph:
                    paragraphs.append(temp_paragraph.strip())
                    temp_paragraph = ""

    if temp_paragraph:
        paragraphs.append(temp_paragraph.strip())

    return paragraphs

def extract_text_from_txt(txt_path):
    """ä» TXT æ–‡ä»¶ä¸­æå–æ–‡æœ¬ï¼ŒæŒ‰ç©ºè¡Œåˆ†æ®µ"""
    paragraphs = []
    temp_paragraph = ""
    
    with open(txt_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line:
                temp_paragraph += " " + line
            else:
                if temp_paragraph:
                    paragraphs.append(temp_paragraph.strip())
                    temp_paragraph = ""
    
    if temp_paragraph:
        paragraphs.append(temp_paragraph.strip())
    
    return paragraphs

def process_document(file_path):
    """å¤„ç†æ–‡æ¡£å¹¶è¿”å›å„ä¸ªæ¨¡å—çš„ç»“æœ"""
    print(f"\nğŸ“„ å¼€å§‹å¤„ç†æ–‡æ¡£ï¼š{os.path.basename(file_path)}")
    
    # æ ¹æ®æ–‡ä»¶æ‰©å±•åé€‰æ‹©ä¸åŒçš„æå–æ–¹æ³•
    file_ext = file_path.lower().split('.')[-1]
    print(f"ğŸ“‘ æ–‡ä»¶ç±»å‹ï¼š{file_ext}")
    
    if file_ext == 'pdf':
        print("âš™ï¸ æ­£åœ¨æå– PDF æ–‡æœ¬...")
        paragraphs = extract_text_from_pdf(file_path)
    elif file_ext in ['doc', 'docx']:
        print("âš™ï¸ æ­£åœ¨æå– Word æ–‡æœ¬...")
        paragraphs = extract_text_from_docx(file_path)
    elif file_ext == 'txt':
        print("âš™ï¸ æ­£åœ¨æå– TXT æ–‡æœ¬...")
        paragraphs = extract_text_from_txt(file_path)
    else:
        print(f"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_ext}")
        raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_ext}")
    
    print(f"âœ… æ–‡æœ¬æå–å®Œæˆï¼Œå…± {len(paragraphs)} ä¸ªæ®µè½")
    
    client = OpenAI(base_url=api_url, api_key=api_key)
    full_text = '\n'.join(paragraphs)
    
    print("\nğŸ”„ å¼€å§‹ç”Ÿæˆæ€ç»´å¯¼å›¾...")
    generator = MindMapGenerator(full_text, client)
    structure = generator.analyze_structure()
    mermaid_code = generator.generate_mermaid()
    mindmap_image_path = generator.export_image()
    print("âœ… æ€ç»´å¯¼å›¾ç”Ÿæˆå®Œæˆ")
    
    '''
    print("\nğŸ”„ å¼€å§‹ç”Ÿæˆç¿»è¯‘...")
    translator = DocumentTranslator(client)
    translation = translator.translate_text(full_text)
    print("âœ… ç¿»è¯‘ç”Ÿæˆå®Œæˆ")
    
    print("\nğŸ”„ å¼€å§‹ç”Ÿæˆé˜…è¯»ç†è§£é¢˜...")
    reading_comprehension = ReadingComprehension(client).generate_questions(full_text)
    print("âœ… é˜…è¯»ç†è§£é¢˜ç”Ÿæˆå®Œæˆ")
    
    print("\nğŸ”„ å¼€å§‹ç”Ÿæˆè¯æ±‡è¡¨...")
    vocabulary = word_extraction(client).extract_keywords(full_text)
    print("âœ… è¯æ±‡è¡¨ç”Ÿæˆå®Œæˆ")
    
    # è¿”å›æ‰€æœ‰ç»“æœ
    result = {
        "mindmap": {
            "mermaid_code": mermaid_code,
            "image_path": mindmap_image_path
        },
        "translation": translation,
        "reading_comprehension": reading_comprehension,
        "vocabulary": vocabulary
    }
    '''
    result = {
        "mindmap": {
            "mermaid_code": mermaid_code,
            "image_path": mindmap_image_path
        },
        "translation": "translation",
        "reading_comprehension": "reading_comprehension",
        "vocabulary": "vocabulary"
    }
    print("\nğŸ‰ æ–‡æ¡£å¤„ç†å…¨éƒ¨å®Œæˆï¼")
    return result

# å¦‚æœç›´æ¥è¿è¡Œæ­¤è„šæœ¬ï¼Œåˆ™ä¿æŒåŸæœ‰åŠŸèƒ½
def save_markdown(md_text, output_path):
    """ä¿å­˜ Markdown æ–‡ä»¶"""
    with open(output_path, "a", encoding="utf-8") as f:
        f.write("\n---\n")  # æ·»åŠ åˆ†éš”ç¬¦
        f.write(md_text)

def main(file_path, output_md_path):
    result = process_document(file_path)
    
    # æ¸…ç©ºè¾“å‡ºæ–‡ä»¶
    with open(output_md_path, "w", encoding="utf-8") as f:
        f.write("")
    
    # ä¿å­˜æ€ç»´å¯¼å›¾
    save_markdown(f"## æ€ç»´å¯¼å›¾\n\n![æ€ç»´å¯¼å›¾]({result['mindmap']['image_path']})\n\n```mermaid\n{result['mindmap']['mermaid_code']}\n```", output_md_path)
    
    # ä¿å­˜ç¿»è¯‘
    save_markdown(result["translation"], output_md_path)
    
    # ä¿å­˜é˜…è¯»ç†è§£é¢˜
    save_markdown(result["reading_comprehension"], output_md_path)
    
    # ä¿å­˜è¯æ±‡è¡¨
    save_markdown(result["vocabulary"], output_md_path)
    
    print(f"Markdown æ–‡ä»¶å·²ç”Ÿæˆï¼š{output_md_path}")

# è¿è¡Œä»£ç 
if __name__ == "__main__":
    if len(sys.argv) != 3:
        print("ä½¿ç”¨æ–¹æ³•: python pdf_loader.py <è¾“å…¥æ–‡ä»¶> <è¾“å‡ºæ–‡ä»¶>")
        sys.exit(1)
    
    input_file = sys.argv[1]
    output_md = sys.argv[2]
    print(input_file)
    print(output_md)
    main(input_file, output_md)
